import json
import os
from pathlib import Path
from subprocess import DEVNULL, Popen

import pytest
from testing_utils import CargoTools

FAILED_CONFIGS = []


# Cmdline options
def pytest_addoption(parser):
    parser.addoption(
        "--traces",
        choices=["none", "target", "all"],
        default="none",
        help="Verbosity of traces in output and HTML report. "
        '"none" - show no traces, '
        '"target" - show traces generated by test code, '
        '"all" - show all traces. ',
    )
    parser.addoption(
        "--target-path",
        type=Path,
        help="Path to test scenarios executable. Search is performed by default.",
    )
    parser.addoption(
        "--target-name",
        type=str,
        default="rust_test_scenarios",
        help='Test scenario executable name. Overwritten by "--target-path".',
    )
    parser.addoption(
        "--build-scenarios",
        action="store_true",
        help="Build test scenarios executables.",
    )
    parser.addoption(
        "--build-scenarios-timeout",
        type=float,
        default=180.0,
        help="Build command timeout in seconds. Default: %(default)s",
    )
    parser.addoption(
        "--default-execution-timeout",
        type=float,
        default=5.0,
        help="Default execution timeout in seconds. Default: %(default)s",
    )


# Hooks
@pytest.hookimpl(tryfirst=True)
def pytest_sessionstart(session: pytest.Session):
    # Executes always as first - tests collection and running.
    try:
        # Build scenarios.
        if session.config.getoption("--build-scenarios"):
            print("Building test scenarios executable...")
            build_timeout = session.config.getoption("--build-scenarios-timeout")
            tools = CargoTools(build_timeout=build_timeout)
            target_name = tools.select_target_path(session.config, expect_exists=False).name
            tools.build(target_name)

    except Exception as e:
        pytest.exit(str(e), returncode=1)


@pytest.fixture(scope="session", autouse=True)
def global_startup(request: pytest.FixtureRequest):
    """
    Executes once per testing session, before any tests. Does not execute on collection.

    Some tests require root permissions, check and authenticate if needed.
    Two modes of operation are supported:
     - `bazel` run - check sudo can be used, skip tests if can't.
     - `pytest` run - prompt for password once collected, run tests as usual.
    """
    root_required_tests = request.config._root_required_tests
    if not root_required_tests:
        return

    # Check for current environment.
    if _inside_bazel_env():
        # Prompt for password is not possible, but user might have root rights.
        # On failure - skip root requiring tests.
        authenticated = _authenticate(interactive=False)
        if not authenticated:
            skipper = pytest.mark.skip(reason="Failed to grant root permissions in Bazel environment.")
            for item in root_required_tests:
                item.add_marker(skipper)

    else:
        # Prompt for password and cache credentials.
        # On failure - exit.
        authenticated = _authenticate(interactive=True)
        if not authenticated:
            pytest.exit("Failed to authenticate", returncode=1)


def _inside_bazel_env() -> bool:
    """
    Return True if running within Bazel environment.
    """
    return os.environ.get("BAZEL_TEST", "") == "1"


def _authenticate(*, interactive: bool) -> bool:
    """
    Authenticate user.
    # Run "sudo -v" and return True if authenticated successfully.

    Parameters
    ----------
    interactive : bool
        Allow password prompt.
    """
    command = ["sudo", "id"] if interactive else ["sudo", "-n", "id"]
    with Popen(command, stdout=DEVNULL, stderr=DEVNULL) as p:
        _, _ = p.communicate()
        return p.returncode == 0


def skip_only_nightly(test_items: list[pytest.Item]):
    """
    Implement only_nightly marker functionality.
    Skip tests marked with 'only_nightly'

    Parameters
    ----------
    test_items : list[pytest.Item]
        List of test items to process.
    """
    for item in test_items:
        if item.get_closest_marker("only_nightly"):
            item.add_marker(pytest.mark.skip(reason="Only for nightly runs"))


def skip_do_not_repeat(test_items: list[pytest.Item], repeat_count: int):
    """
    Implement do_not_repeat marker functionality.
    Skip repetitions of tests marked with 'do_not_repeat'.

    Parameters
    ----------
    test_items : list[pytest.Item]
        List of test items to process.
    repeat_count : int
        Number of times tests are repeated.
    """
    # Start items iteration after first loop and skip tests marked with 'do_not_repeat',
    # so they are executed only once
    for item in test_items[len(test_items) // repeat_count :]:
        if item.get_closest_marker("do_not_repeat"):
            item.add_marker(pytest.mark.skip(reason="Marked as do_not_repeat"))


def pytest_collection_modifyitems(session: pytest.Session, config: pytest.Config, items: list[pytest.Item]):
    # Skip tests marked with 'only_nightly' if NIGHTLY env var is not set to TRUE
    if os.getenv("NIGHTLY", "").lower() not in ("true", "1"):
        skip_only_nightly(items)

    # Skip repetitions of tests marked with 'do_not_repeat'
    count = getattr(config.option, "count", 1)
    skip_do_not_repeat(items, count)


def pytest_collection_finish(session: pytest.Session):
    # Certain tests require root permissions. Collect them and store in config.
    root_required_tests = []
    for item in session.items:
        if "root_required" in item.keywords:
            root_required_tests.append(item)

    session.config._root_required_tests = root_required_tests


def pytest_html_report_title(report):
    # Change report title
    report.title = "Component Integration Tests Report"


def pytest_html_results_table_header(cells):
    # Create additional table headers
    cells.insert(1, "<th>Test Input</th>")
    cells.insert(2, "<th>Description</th>")
    cells.insert(3, "<th>Test Scenario Name</th>")
    cells.insert(4, "<th>Test Scenario Command</th>")


def pytest_html_results_table_row(report, cells):
    # Create additional table columns with TC __doc__ and execution date
    cells.insert(
        1,
        f'<td><pre style="white-space:pre-wrap;word-wrap:break-word">{json.dumps(report.input)}</pre></td>',
    )
    cells.insert(2, f"<td><pre>{report.description}</pre></td>")
    cells.insert(3, f"<td><pre>{report.scenario}</pre></td>")
    cells.insert(4, f"<td><pre>{report.command}</pre></td>")


@pytest.hookimpl(hookwrapper=True)
def pytest_runtest_makereport(item, call):
    # Extract TC's data
    outcome = yield
    report = outcome.get_result()
    report.description = str(item.function.__doc__)
    report.scenario = item.funcargs.get("scenario_name", "")
    report.input = item.funcargs.get("test_config", "")

    command = []
    for token in item.funcargs.get("command", ""):
        if " " in token:
            command.append(f"'{token}'")
        else:
            command.append(token)
    report.command = " ".join(command)
    # If bazel is used, modify command
    if "BAZEL_VERSION" in os.environ:
        report.command = report.command.replace(
            "component_integration_tests/rust_test_scenarios/rust_test_scenarios",
            "bazel run //component_integration_tests/rust_test_scenarios:rust_test_scenarios --",
        )

    # Store failed command for printing in summary
    if report.failed:
        FAILED_CONFIGS.append(
            {
                "nodeid": report.nodeid,
                "command": report.command,
            }
        )


def pytest_terminal_summary(terminalreporter):
    if not FAILED_CONFIGS:
        return
    # Print failed scenarios info
    terminalreporter.write_sep("=", "Failed tests reproduction info")
    terminalreporter.write_line("Run failed scenarios from the repo root working directory\n")

    for entry in FAILED_CONFIGS:
        terminalreporter.write_line(f"{entry['nodeid']} | Run command:\n{entry['command']}\n")
